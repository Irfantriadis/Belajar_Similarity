{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Memuat Data\n",
    "df = pd.read_csv('dataset_bersih.csv')\n",
    "tweets = df['Tweet']\n",
    "sentiments = df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>undang hitamputih menang ssk jakarta mjkt laya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>selamat buka puasa moga amal ibadah ni terima ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>nih trans hitam putih dapat harga norwegia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>selamat ya mas masuk hitamputih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>asiknya nonton hitam putih trans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>banget deh gw kesel klo orang debat pake emosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>orang miskin miskin klo sekolah pungut liar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>tidak emosi cepat tua nonton lihat emosi bicara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>dari tampil kyk preman tau bkin kisruh usak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>aja tidak berbelitbelit muter tidak buang tida...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                              Tweet\n",
       "0            1  undang hitamputih menang ssk jakarta mjkt laya...\n",
       "1            1  selamat buka puasa moga amal ibadah ni terima ...\n",
       "2            1         nih trans hitam putih dapat harga norwegia\n",
       "3            1                    selamat ya mas masuk hitamputih\n",
       "4            1                   asiknya nonton hitam putih trans\n",
       "..         ...                                                ...\n",
       "395          0  banget deh gw kesel klo orang debat pake emosi...\n",
       "396          0        orang miskin miskin klo sekolah pungut liar\n",
       "397          0    tidak emosi cepat tua nonton lihat emosi bicara\n",
       "398          0        dari tampil kyk preman tau bkin kisruh usak\n",
       "399          0  aja tidak berbelitbelit muter tidak buang tida...\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      undang hitamputih menang ssk jakarta mjkt laya...\n",
       "1      selamat buka puasa moga amal ibadah ni terima ...\n",
       "2             nih trans hitam putih dapat harga norwegia\n",
       "3                        selamat ya mas masuk hitamputih\n",
       "4                       asiknya nonton hitam putih trans\n",
       "                             ...                        \n",
       "395    banget deh gw kesel klo orang debat pake emosi...\n",
       "396          orang miskin miskin klo sekolah pungut liar\n",
       "397      tidak emosi cepat tua nonton lihat emosi bicara\n",
       "398          dari tampil kyk preman tau bkin kisruh usak\n",
       "399    aja tidak berbelitbelit muter tidak buang tida...\n",
       "Name: Tweet, Length: 400, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "395    0\n",
       "396    0\n",
       "397    0\n",
       "398    0\n",
       "399    0\n",
       "Name: Sentiment, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 837)\t0.3297293508924859\n",
      "  (0, 566)\t0.3524071845138774\n",
      "  (0, 674)\t0.3524071845138774\n",
      "  (0, 430)\t0.31363917187471896\n",
      "  (0, 998)\t0.3524071845138774\n",
      "  (0, 643)\t0.31363917187471896\n",
      "  (0, 381)\t0.2909613382533275\n",
      "  (0, 1118)\t0.48714322682588923\n",
      "  (1, 36)\t0.3412930465928802\n",
      "  (1, 1057)\t0.29193832549405796\n",
      "  (1, 729)\t0.3277121150652583\n",
      "  (1, 390)\t0.3412930465928802\n",
      "  (1, 38)\t0.38347927309284935\n",
      "  (1, 675)\t0.26955791149383634\n",
      "  (1, 845)\t0.38347927309284935\n",
      "  (1, 181)\t0.3277121150652583\n",
      "  (1, 939)\t0.31661568604346907\n",
      "  (2, 738)\t0.4969817452312205\n",
      "  (2, 363)\t0.42470858350436247\n",
      "  (2, 231)\t0.44230920895154013\n",
      "  (2, 853)\t0.26473677413902375\n",
      "  (2, 380)\t0.2606909746653215\n",
      "  (2, 1083)\t0.31940931041870413\n",
      "  (2, 730)\t0.37003604722468214\n",
      "  (3, 626)\t0.45348145917408134\n",
      "  :\t:\n",
      "  (396, 672)\t0.6783652250513932\n",
      "  (396, 525)\t0.305333965178728\n",
      "  (396, 765)\t0.22368295680594222\n",
      "  (396, 937)\t0.3256856606186563\n",
      "  (397, 287)\t0.6918148643195156\n",
      "  (397, 1088)\t0.3208964262065372\n",
      "  (397, 202)\t0.3636530232916262\n",
      "  (397, 149)\t0.3113876830526774\n",
      "  (397, 581)\t0.3113876830526774\n",
      "  (397, 1067)\t0.2054348132254074\n",
      "  (397, 737)\t0.2237098902742466\n",
      "  (398, 524)\t0.3812741553230908\n",
      "  (398, 160)\t0.3812741553230908\n",
      "  (398, 1031)\t0.3567386968009245\n",
      "  (398, 1127)\t0.32582767473431873\n",
      "  (398, 835)\t0.3567386968009245\n",
      "  (398, 1040)\t0.3567386968009245\n",
      "  (398, 554)\t0.33933051194096225\n",
      "  (398, 233)\t0.32582767473431873\n",
      "  (399, 179)\t0.38474658089599006\n",
      "  (399, 692)\t0.38474658089599006\n",
      "  (399, 135)\t0.38474658089599006\n",
      "  (399, 693)\t0.34242093894965975\n",
      "  (399, 19)\t0.25776965505699917\n",
      "  (399, 1067)\t0.6100925429532515\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/sentiments.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(vectorizer, 'model/tfidf_vectorizer_similarity.pkl')\n",
    "joblib.dump(X, 'model/tweet_vectors.pkl')\n",
    "joblib.dump(sentiments, 'model/sentiments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Memuat model dan data yang sudah disimpan\n",
    "vectorizer = joblib.load('model/tfidf_vectorizer_similarity.pkl')\n",
    "tweet_vectors = joblib.load('model/tweet_vectors.pkl')\n",
    "sentiments = joblib.load('model/sentiments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def predict_sentiment_similarity(text):\n",
    "    # Mengubah teks input menjadi vektor menggunakan vectorizer yang sudah dilatih\n",
    "    text_vector = vectorizer.transform([text.lower()])\n",
    "    \n",
    "    # Menghitung cosine similarity antara teks input dengan tweet yang ada di dataset\n",
    "    similarities = cosine_similarity(text_vector, tweet_vectors)\n",
    "    \n",
    "    # Menemukan tweet yang paling mirip\n",
    "    most_similar_index = similarities.argmax()\n",
    "    similarity_score = similarities[0, most_similar_index]\n",
    "    \n",
    "    # Mendapatkan sentimen dari tweet yang paling mirip\n",
    "    sentiment = sentiments.iloc[most_similar_index]\n",
    "    \n",
    "    # Menampilkan tweet yang paling mirip\n",
    "    most_similar_tweet = tweets.iloc[most_similar_index]\n",
    "    \n",
    "    return sentiment, similarity_score, most_similar_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimen: Negatif (Similarity: 0.34)\n",
      "User Input: acara sampah\n",
      "Tweet yang paling mirip: mata najwa acara sampah yang cuman keruh suasana gue sbg ajar mending pilih ilc\n"
     ]
    }
   ],
   "source": [
    "# Mengambil input tweet dari pengguna\n",
    "user_input = input(\"Masukkan teks tweet: \")\n",
    "predicted_sentiment, similarity_score, most_similar_tweet = predict_sentiment_similarity(user_input)\n",
    "\n",
    "# Menampilkan hasil prediksi dan tweet yang paling mirip\n",
    "if predicted_sentiment == 1:\n",
    "    print(f\"Sentimen: Positif (Similarity: {similarity_score:.2f})\")\n",
    "else:\n",
    "    print(f\"Sentimen: Negatif (Similarity: {similarity_score:.2f})\")\n",
    "\n",
    "print(f\"User Input: {user_input}\")\n",
    "print(f\"Tweet yang paling mirip: {most_similar_tweet}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
